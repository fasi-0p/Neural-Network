import tensorflow as tf
import mlflow
import pandas as pd
import numpy as np
from hyperopt import STATUS_OK, Trials, fmin, hp, tpe
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler  # Import StandardScaler
from mlflow.models import infer_signature

# Loading and preprocessing data
df = pd.read_csv('Tesla_Nasdaq_Prediction.csv')
df.drop(['Date'], axis=1, inplace=True)
scaler = StandardScaler()
for i in df:
    df[[i]] = scaler.fit_transform(df[[i]])

x = df.iloc[:, 1:]
y = df.iloc[:, :1]

# Ensure the input shape for LSTM is 3D
x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.75)

# Reshape x_train and x_test for LSTM
x_train = np.reshape(x_train.values, (x_train.shape[0], 1, x_train.shape[1]))
x_test = np.reshape(x_test.values, (x_test.shape[0], 1, x_test.shape[1]))

sign = infer_signature(x_train, y_train)

# Hyperparameter search space definition using Hyperopt
params_space = {
    'sequence': hp.choice('sequence', [20, 44, 66]),
    'act': hp.choice('act', ['tanh', 'relu']),
    'opt': hp.choice('opt', ['adam', 'rmsprop']),
    'loss': 'mse'
}

# Train model function
def train_model(params, epochs, x_train, y_train, x_test, y_test):
    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001) if params['opt'] == 'adam' else tf.keras.optimizers.RMSprop(learning_rate=0.001)

    model = tf.keras.Sequential([
        tf.keras.Input(shape=(1, x_train.shape[2])),
        tf.keras.layers.LSTM(64, activation=params['act'], return_sequences=False),
        tf.keras.layers.Dense(64, activation=params['act']),
        tf.keras.layers.Dense(1)
    ])

    # Compile model with the chosen parameters
    model.compile(
        optimizer=optimizer,
        loss=params['loss'],
        metrics=[tf.keras.metrics.RootMeanSquaredError()]
    )

    # mlflow tracking
    with mlflow.start_run(nested=True):
        model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=epochs, batch_size=20)
        
        # Evaluate the model
        eval_result = model.evaluate(x_test, y_test, batch_size=20)
        eval_rmse = eval_result[1]

        # Log parameters and metrics
        mlflow.log_param('sequence', params['sequence'])
        mlflow.log_param('activation', params['act'])
        mlflow.log_param('optimizer', params['opt'])
        mlflow.log_param('loss', params['loss'])
        mlflow.log_metric('eval_rmse', eval_rmse)

        # Log the model
        mlflow.tensorflow.log_model(model, "model", signature=sign)

        return {"loss": eval_rmse, "status": STATUS_OK, "model": model}

# Hyperparameter optimization objective function
def objective(params):
    result = train_model(
        params, epochs=3, x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test)
    return result

# Set MLflow experiment
mlflow.set_experiment("LSTM based Tesla Stock2")

# Hyperparameter optimization using Hyperopt
with mlflow.start_run():
    trials = Trials()
    best = fmin(
        fn=objective,
        space=params_space,
        algo=tpe.suggest,
        max_evals=4,
        trials=trials
    )

    # Fetch the details of the best run
    best_run = sorted(trials.results, key=lambda x: x['loss'])[0]  # Get the best run

    # Log the best parameters, loss, and model
    mlflow.log_params(best)
    mlflow.log_metric('eval_rmse', best_run['loss'])
    mlflow.tensorflow.log_model(best_run['model'], 'model', signature=sign)

    # Print the best parameters and corresponding loss
    print(f"Best Parameters: {best}")
    print(f"Best eval rmse?: {best_run['loss']}")
