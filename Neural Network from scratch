import numpy as np

class NeuralNetwork:
  def __init__(self,input_size,hidden_size,output_size):
    self.input_size=input_size
    self.hidden_size=hidden_size
    self.output_size=output_size

    #Initialization of Wieghts and Biases
    self.weights_input_hidden=np.random.rand(self.input_size,self.hidden_size)
    self.weights_hidden_output=np.random.rand(self.hidden_size,self.output_size)
    self.bias_input_hidden=np.random.rand(1,self.hidden_size)
    self.bias_output_hidden=np.random.rand(1,self.output_size)

  def sigmoid(self,x):
    return 1 / (1 + np.exp(-x))

  def sigmoid_derivative(self,x):
    return x * (1 - x)

  def MSR(actual,predicted):
    return np.mean((actual-predicted)**2)

  def Feedforward(self, x):
    self.input_to_hidden=self.sigmoid((np.dot(x, self.weights_input_hidden) + self.bias_input_hidden)) # activation(data*weights+bias) for input to hidden
    self.hidden_to_output=self.sigmoid((np.dot(self.input_to_hidden, self.weights_hidden_output) + self.bias_output_hidden)) # activation(data*weights+bias) for hidden to output

    return self.hidden_to_output

  def BackPropogation(self,x,y,learning_rate):  #x=input y=output
    error= y- self.hidden_to_output
    tweaking=error*self.sigmoid_derivative(self.hidden_to_output)

    error_hidden=np.dot(tweaking,self.weights_hidden_output.T)
    tweaking_hidden=error_hidden*self.sigmoid_derivative(self.hidden_to_output)

    #updating weights
    #self.weights_hidden_output += self.hidden_to_output.T.dot(tweaking_hidden) * learning_rate  # Hidden to output weights update. wrong logic hence commented
    self.weights_hidden_output += self.input_to_hidden.T.dot(tweaking) * learning_rate  # input to hidden weights update
    self.weights_input_hidden += x.T.dot(tweaking_hidden) * learning_rate  # Input to hidden weights update

  def train(self,x,y,learning_rate=0.1, epochs=1000):
    for epoch in range(epochs):
      output=self.Feedforward(x)
      self.BackPropogation(x,y,0.1)

      if ((epoch%100)==0):
        loss=np.mean((y-self.hidden_to_output)**2) #MSR
        print(f"Epoch:{epoch} | Loss:{loss}")






#*********** Personal Note/ Algo ******************************#


'''
first we need the number of input, and number of rows of dataset, and number of neurons required for output
wrt to that we inititalise weight for the input-to-hidden in the shape of (number of features,number of rows of data)  +  bias(1,number of hidden neuron)
                                  for the hidden-to-output in the shape of (number of neurons in hidden, result)     +   bias(1, output neurons)

in feed forward:
you multiply the data with the weights set in constructor and also add bias, and provide it as an input for hidden,
then an activation function is applied whose function needs to be defined

then in hidden layer you multiply the previous output with weights of hidden-to-output, add bias
apply an activation function. at the end return the calculated_output


in backpropogation:
calculate error ie og_output - calculated output
calculate the solution_for_tweaking ie error * sigmoid_derivative(calculated_output)

now to calculate the error in hidden layer we multiply the solution_for_tweaking * weights of hidden-to-output
now multiply the above thing with sigmoid_derivative(hidden_output)

we update weights using gradient descent and then update biases


train:
then run a forloop wrt epochs and also have a parameter of learning rate
'''


